#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨ç”Ÿæˆ pages.json æ–‡ä»¶çš„è„šæœ¬
éå† public ç›®å½•ä¸‹çš„æ‰€æœ‰ HTML æ–‡ä»¶ï¼Œæå–æ ‡é¢˜å’Œå…ƒæ•°æ®ï¼Œç”Ÿæˆ JSON é…ç½®æ–‡ä»¶
"""

import os
import json
import re
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup

def extract_title_from_html(file_path):
    """ä»HTMLæ–‡ä»¶ä¸­æå–æ ‡é¢˜"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
            soup = BeautifulSoup(content, 'html.parser')
            title_tag = soup.find('title')
            if title_tag:
                return title_tag.get_text().strip()
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    return None

def extract_date_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ"""
    # åŒ¹é… YYYY-MM-DD æ ¼å¼çš„æ—¥æœŸ
    date_pattern = r'(\d{4}-\d{2}-\d{2})'
    match = re.search(date_pattern, filename)
    if match:
        return match.group(1)
    return None

def get_category_info(category_name):
    """è·å–åˆ†ç±»çš„æ˜¾ç¤ºåç§°å’Œå›¾æ ‡"""
    category_mapping = {
        'culture': {
            'name': 'æ–‡åŒ– Culture',
            'icon': 'ğŸ¨'
        },
        'history': {
            'name': 'å†å² History',
            'icon': 'ğŸ“š'
        },
        'politics': {
            'name': 'æ”¿æ²» Politics',
            'icon': 'ğŸ›ï¸'
        },
        'economy': {
            'name': 'ç»æµ Economy',
            'icon': 'ğŸ’°'
        },
        'science': {
            'name': 'ç§‘å­¦ Science',
            'icon': 'ğŸ”¬'
        },
        'art': {
            'name': 'è‰ºæœ¯ Art',
            'icon': 'ğŸ­'
        },
        'literature': {
            'name': 'æ–‡å­¦ Literature',
            'icon': 'ğŸ“–'
        }
    }
    
    return category_mapping.get(category_name, {
        'name': category_name.capitalize(),
        'icon': 'ğŸ“„'
    })

def scan_public_directory():
    """æ‰«æ public ç›®å½•ï¼Œç”Ÿæˆé¡µé¢æ•°æ®"""
    public_dir = Path('public')
    
    if not public_dir.exists():
        print("é”™è¯¯: public ç›®å½•ä¸å­˜åœ¨")
        return None
    
    categories = {}
    
    # éå† public ç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
    for category_dir in public_dir.iterdir():
        if category_dir.is_dir() and category_dir.name != '__pycache__':
            category_name = category_dir.name
            category_info = get_category_info(category_name)
            
            articles = []
            
            # éå†åˆ†ç±»ç›®å½•ä¸‹çš„æ‰€æœ‰ HTML æ–‡ä»¶
            for html_file in category_dir.glob('*.html'):
                title = extract_title_from_html(html_file)
                date = extract_date_from_filename(html_file.name)
                
                if title and date:
                    # è®¡ç®—ç›¸å¯¹äº public ç›®å½•çš„è·¯å¾„
                    relative_path = f"{category_name}/{html_file.name}"
                    
                    article = {
                        'title': title,
                        'url': relative_path,
                        'date': date
                    }
                    articles.append(article)
                else:
                    print(f"è­¦å‘Š: æ— æ³•æå– {html_file} çš„æ ‡é¢˜æˆ–æ—¥æœŸ")
            
            # æŒ‰æ—¥æœŸå€’åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
            articles.sort(key=lambda x: x['date'], reverse=True)
            
            if articles:  # åªæ·»åŠ æœ‰æ–‡ç« çš„åˆ†ç±»
                categories[category_name] = {
                    'name': category_info['name'],
                    'icon': category_info['icon'],
                    'articles': articles
                }
    
    return {
        'categories': categories
    }

def generate_pages_json():
    """ç”Ÿæˆ pages.json æ–‡ä»¶"""
    print("å¼€å§‹æ‰«æ public ç›®å½•...")
    
    pages_data = scan_public_directory()
    
    if pages_data is None:
        return False
    
    # ç”Ÿæˆ JSON æ–‡ä»¶
    output_file = Path('public/pages.json')
    
    try:
        with open(output_file, 'w', encoding='utf-8') as file:
            json.dump(pages_data, file, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {output_file}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        total_articles = sum(len(cat['articles']) for cat in pages_data['categories'].values())
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - åˆ†ç±»æ•°é‡: {len(pages_data['categories'])}")
        print(f"   - æ–‡ç« æ€»æ•°: {total_articles}")
        
        for category_name, category_data in pages_data['categories'].items():
            print(f"   - {category_data['name']}: {len(category_data['articles'])} ç¯‡æ–‡ç« ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

if __name__ == '__main__':
    print("ğŸš€ è‡ªåŠ¨ç”Ÿæˆ pages.json æ–‡ä»¶")
    print("=" * 50)
    
    success = generate_pages_json()
    
    if success:
        print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    else:
        print("\nâŒ ä»»åŠ¡å¤±è´¥ï¼")
