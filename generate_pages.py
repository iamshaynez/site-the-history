#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨ç”Ÿæˆ pages.json æ–‡ä»¶çš„è„šæœ¬
éå† public ç›®å½•ä¸‹çš„æ‰€æœ‰ HTML æ–‡ä»¶ï¼Œæå–æ ‡é¢˜å’Œå…ƒæ•°æ®ï¼Œç”Ÿæˆ JSON é…ç½®æ–‡ä»¶
"""

import os
import json
import re
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom

def extract_title_from_html(file_path):
    """ä»HTMLæ–‡ä»¶ä¸­æå–æ ‡é¢˜"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
            soup = BeautifulSoup(content, 'html.parser')
            title_tag = soup.find('title')
            if title_tag:
                return title_tag.get_text().strip()
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    return None

def extract_date_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ"""
    # åŒ¹é… YYYY-MM-DD æ ¼å¼çš„æ—¥æœŸ
    date_pattern = r'(\d{4}-\d{2}-\d{2})'
    match = re.search(date_pattern, filename)
    if match:
        return match.group(1)
    return None

def get_category_info(category_name):
    """è·å–åˆ†ç±»çš„æ˜¾ç¤ºåç§°å’Œå›¾æ ‡"""
    category_mapping = {
        'culture': {
            'name': 'æ–‡åŒ– Culture',
            'icon': 'ğŸ¨'
        },
        'history': {
            'name': 'å†å² History',
            'icon': 'ğŸ“š'
        },
        'politics': {
            'name': 'æ”¿æ²» Politics',
            'icon': 'ğŸ›ï¸'
        },
        'economy': {
            'name': 'ç»æµ Economy',
            'icon': 'ğŸ’°'
        },
        'science': {
            'name': 'ç§‘å­¦ Science',
            'icon': 'ğŸ”¬'
        },
        'art': {
            'name': 'è‰ºæœ¯ Art',
            'icon': 'ğŸ­'
        },
        'literature': {
            'name': 'æ–‡å­¦ Literature',
            'icon': 'ğŸ“–'
        }
    }
    
    return category_mapping.get(category_name, {
        'name': category_name.capitalize(),
        'icon': 'ğŸ“„'
    })

def scan_public_directory():
    """æ‰«æ public ç›®å½•ï¼Œç”Ÿæˆé¡µé¢æ•°æ®"""
    public_dir = Path('public')
    
    if not public_dir.exists():
        print("é”™è¯¯: public ç›®å½•ä¸å­˜åœ¨")
        return None
    
    categories = {}
    
    # éå† public ç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
    for category_dir in public_dir.iterdir():
        if category_dir.is_dir() and category_dir.name != '__pycache__':
            category_name = category_dir.name
            category_info = get_category_info(category_name)
            
            articles = []
            
            # éå†åˆ†ç±»ç›®å½•ä¸‹çš„æ‰€æœ‰ HTML æ–‡ä»¶
            for html_file in category_dir.glob('*.html'):
                title = extract_title_from_html(html_file)
                date = extract_date_from_filename(html_file.name)
                
                if title and date:
                    # è®¡ç®—ç›¸å¯¹äº public ç›®å½•çš„è·¯å¾„
                    relative_path = f"{category_name}/{html_file.name}"
                    
                    article = {
                        'title': title,
                        'url': relative_path,
                        'date': date
                    }
                    articles.append(article)
                else:
                    print(f"è­¦å‘Š: æ— æ³•æå– {html_file} çš„æ ‡é¢˜æˆ–æ—¥æœŸ")
            
            # æŒ‰æ—¥æœŸå€’åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
            articles.sort(key=lambda x: x['date'], reverse=True)
            
            if articles:  # åªæ·»åŠ æœ‰æ–‡ç« çš„åˆ†ç±»
                categories[category_name] = {
                    'name': category_info['name'],
                    'icon': category_info['icon'],
                    'articles': articles
                }
    
    return {
        'categories': categories
    }

def generate_pages_json():
    """ç”Ÿæˆ pages.json æ–‡ä»¶"""
    print("å¼€å§‹æ‰«æ public ç›®å½•...")
    
    pages_data = scan_public_directory()
    
    if pages_data is None:
        return False
    
    # ç”Ÿæˆ JSON æ–‡ä»¶
    output_file = Path('public/pages.json')
    
    try:
        with open(output_file, 'w', encoding='utf-8') as file:
            json.dump(pages_data, file, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {output_file}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        total_articles = sum(len(cat['articles']) for cat in pages_data['categories'].values())
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - åˆ†ç±»æ•°é‡: {len(pages_data['categories'])}")
        print(f"   - æ–‡ç« æ€»æ•°: {total_articles}")
        
        for category_name, category_data in pages_data['categories'].items():
            print(f"   - {category_data['name']}: {len(category_data['articles'])} ç¯‡æ–‡ç« ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def generate_sitemap_xml(pages_data, base_url="https://your-domain.com"):
    """ç”Ÿæˆ sitemap.xml æ–‡ä»¶"""
    print("å¼€å§‹ç”Ÿæˆ sitemap.xml...")
    
    # åˆ›å»ºæ ¹å…ƒç´ 
    urlset = Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    
    # æ·»åŠ é¦–é¡µ
    url_elem = SubElement(urlset, 'url')
    SubElement(url_elem, 'loc').text = base_url
    SubElement(url_elem, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
    SubElement(url_elem, 'changefreq').text = 'daily'
    SubElement(url_elem, 'priority').text = '1.0'
    
    # ä¸ºæ¯ä¸ªæ–‡ç« æ·»åŠ  URL
    for category_name, category_data in pages_data['categories'].items():
        for article in category_data['articles']:
            url_elem = SubElement(urlset, 'url')
            
            # æ„å»ºå®Œæ•´ URL
            full_url = f"{base_url.rstrip('/')}/{article['url']}"
            SubElement(url_elem, 'loc').text = full_url
            
            # ä½¿ç”¨æ–‡ç« æ—¥æœŸä½œä¸ºæœ€åä¿®æ”¹æ—¶é—´
            SubElement(url_elem, 'lastmod').text = article['date']
            
            # è®¾ç½®æ›´æ–°é¢‘ç‡å’Œä¼˜å…ˆçº§
            SubElement(url_elem, 'changefreq').text = 'weekly'
            SubElement(url_elem, 'priority').text = '0.8'
    
    # ç”Ÿæˆæ ¼å¼åŒ–çš„ XML
    rough_string = tostring(urlset, 'utf-8')
    reparsed = minidom.parseString(rough_string)
    pretty_xml = reparsed.toprettyxml(indent="  ", encoding='utf-8').decode('utf-8')
    
    # ç§»é™¤ç©ºè¡Œ
    pretty_xml = '\n'.join([line for line in pretty_xml.split('\n') if line.strip()])
    
    # ä¿å­˜æ–‡ä»¶
    output_file = Path('public/sitemap.xml')
    
    try:
        with open(output_file, 'w', encoding='utf-8') as file:
            file.write(pretty_xml)
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {output_file}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_urls = 1 + sum(len(cat['articles']) for cat in pages_data['categories'].values())
        print(f"ğŸ“Š Sitemap ç»Ÿè®¡:")
        print(f"   - æ€» URL æ•°é‡: {total_urls}")
        print(f"   - é¦–é¡µ: 1 ä¸ª")
        
        for category_name, category_data in pages_data['categories'].items():
            print(f"   - {category_data['name']}: {len(category_data['articles'])} ä¸ªé¡µé¢")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ sitemap.xml æ—¶å‡ºé”™: {e}")
        return False

def generate_robots_txt(base_url="https://your-domain.com"):
    """ç”Ÿæˆ robots.txt æ–‡ä»¶"""
    print("å¼€å§‹ç”Ÿæˆ robots.txt...")
    
    robots_content = f"""User-agent: *
Allow: /

# Sitemap
Sitemap: {base_url.rstrip('/')}/sitemap.xml

# ç¦æ­¢è®¿é—®çš„ç›®å½•
Disallow: /admin/
Disallow: /private/
Disallow: /*.json$
"""
    
    output_file = Path('public/robots.txt')
    
    try:
        with open(output_file, 'w', encoding='utf-8') as file:
            file.write(robots_content)
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ robots.txt æ—¶å‡ºé”™: {e}")
        return False

if __name__ == '__main__':
    print("ğŸš€ è‡ªåŠ¨ç”Ÿæˆ pages.json å’Œ sitemap.xml æ–‡ä»¶")
    print("=" * 60)
    
    # ç”Ÿæˆ pages.json
    success_pages = generate_pages_json()
    
    if success_pages:
        # é‡æ–°è¯»å–ç”Ÿæˆçš„ pages.json æ•°æ®
        try:
            with open('public/pages.json', 'r', encoding='utf-8') as file:
                pages_data = json.load(file)
            
            print("\n" + "=" * 60)
            
            # ç”Ÿæˆ sitemap.xml
            base_url = input("è¯·è¾“å…¥ç½‘ç«™çš„åŸºç¡€ URL (ä¾‹å¦‚: https://your-domain.com): ").strip()
            if not base_url:
                base_url = "https://learn.xiaowenz.com"
                print(f"ä½¿ç”¨é»˜è®¤ URL: {base_url}")
            
            success_sitemap = generate_sitemap_xml(pages_data, base_url)
            
            # ç”Ÿæˆ robots.txt
            success_robots = generate_robots_txt(base_url)
            
            print("\n" + "=" * 60)
            print("ğŸ“‹ å®Œæˆæ€»ç»“:")
            print(f"   âœ… pages.json: {'æˆåŠŸ' if success_pages else 'å¤±è´¥'}")
            print(f"   âœ… sitemap.xml: {'æˆåŠŸ' if success_sitemap else 'å¤±è´¥'}")
            print(f"   âœ… robots.txt: {'æˆåŠŸ' if success_robots else 'å¤±è´¥'}")
            
            if success_pages and success_sitemap and success_robots:
                print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
            else:
                print("\nâš ï¸  éƒ¨åˆ†ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ï¼")
                
        except Exception as e:
            print(f"\nâŒ è¯»å– pages.json æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    else:
        print("\nâŒ pages.json ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡ sitemap.xml ç”Ÿæˆï¼")
